# Glossary: Plain-Language AI Safety Terms

This glossary is written for beginners, parents, students,
and anyone who wants simple words for complex ideas.

---

### Existential risk

A risk that could:
- End human civilization, or
- Permanently damage our long-term future.

Not “this product might fail,” but “we might not get a second chance.”

---

### Alignment

Making an AI system’s goals and behavior match what humans actually want,
not just what we typed into the prompt or reward function.

---

### Misalignment

When an AI system is “doing its job” according to its rules,
but the result is bad for humans or not what we really wanted.

---

### Specification gaming

When an AI system finds loopholes in the rules, reward, or task description,
and “wins” in a way that technically follows the instructions
but clearly breaks the spirit of what humans intended.

---

### Capability

What a system can do:
- How well it can reason, plan, write, code, or act.
- How fast it can do those things.
- How many places it can reach (tools, APIs, systems).

Higher capability means more power—and more potential for harm if misused or misaligned.

---

### Agentic AI

AI systems that can:
- Take actions on their own,
- Call tools or other systems,
- Work toward goals over time,
instead of just answering a single question and stopping.

---

### Governance

The rules, processes, and oversight we use to control AI systems:
- Laws and regulations
- Company policies
- Audits, testing, and review
- Who is responsible when something goes wrong

---

### Kill switch / shutdown mechanism

A clear, easy way for humans to stop a system quickly
if it starts doing something unsafe.

If there is no simple way to stop it, that is a safety problem.

---
