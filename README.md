<p align="center">
  <img src="assets/aqs-existential risk-playground-github-banner.png" alt="AQ’s Corner – Existential Risk Playground Banner" width="100%">
</p>

# existential-risk-playground

Tiny, friendly demos that help people understand **existential risk in AI**, **misalignment**, and **AI safety** without needing a PhD.

This repo is for:
- People who keep hearing phrases like *“existential risk”* and *“AI alignment”* and want clear, simple explanations.
- Parents, students, small-business owners, and beginners who want to see **examples**, not just scary headlines.
- Recruiters, hiring managers, or collaborators who want to see how I think about AI risk and how I explain it to non-technical audiences.

---

## What’s inside

### `examples/`

Small, self-contained examples that show ideas like misalignment and “specification gaming” in a safe, playful way.

- `examples/misalignment/`
  - Stories and tiny code demos that show what happens when an AI system optimizes the **wrong thing**, even if it’s “doing what it was told.”
- `examples/specification_gaming/`
  - Simple scenarios where a system finds loopholes in its instructions and “wins” in a way humans did not intend.
- `examples/safety_patterns/`
  - Checklists and patterns that show how to think about guardrails, oversight, and governance.

### `docs/`

Short, beginner-friendly explainers.

- `glossary.md` — Plain-language definitions for words like *existential risk*, *alignment*, *misalignment*, *capability*, etc.
- `teachers_guide.md` — How educators, parents, or workshop leaders can use these examples to teach AI risk.

---

## Who made this?

**Author:** Aqueelah Emanuel  
Cybersecurity Analyst • AI Safety & Governance • Founder, AQ’s Corner LLC

I work at the intersection of cybersecurity, AI safety, and real people’s lives—parents, kids, schools, seniors, and small businesses. This repo is part of my effort to make high-level AI risk concepts understandable and teachable.

---

## How to use this repo

You can:

- **Read the stories** in `examples/` to understand the concepts.
- **Run the tiny Python demos** (they are deliberately very small and safe).
- **Reuse the explanations** in talks, workshops, or blog posts (with attribution).
- **Fork this repo** and add your own examples.

Everything here is educational and non-destructive. There is **no real-world attack code** and no harmful automation. This is a playground for thinking, not for breaking things.

---

## Safety note

Real existential risk from AI involves large, powerful systems with complex behaviors and real-world access.  
This repo does **not** reproduce that. Instead, it uses small, simple examples to **illustrate the ideas** in a way that’s safe, understandable, and shareable.

If you work with production AI systems, you should also consult:
- NIST AI Risk Management Framework
- CISA Secure-by-Design principles
- Your organization’s AI and security policies

This repo is a starting point, not the finish line.

---
